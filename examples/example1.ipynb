{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772f7f04",
   "metadata": {},
   "source": [
    "## First test to the simplegrad library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c932d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=2.0, grad=0.0, op=, label=x)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from simplegrad import Tensor\n",
    "x= Tensor(2.0, label=\"x\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a9ce5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: Tensor(data=[[3. 3. 3.]], grad=[[0. 0. 0.]], op=+, label=)\n",
      "d: Tensor(data=[[2. 2. 0.]], grad=[[0. 0. 0.]], op=*, label=)\n",
      "e: Tensor(data=4.0, grad=0.0, op=sum, label=)\n",
      "grad a: [[2. 1. 0.]]\n",
      "grad b: [[1. 2. 3.]]\n"
     ]
    }
   ],
   "source": [
    "from simplegrad import Tensor\n",
    "# --------------------------\n",
    "# Tensor operations\n",
    "# --------------------------\n",
    "a = Tensor([[1, 2, 3]], requires_grad=True)\n",
    "b = Tensor([[2, 1, 0]], requires_grad=True)\n",
    "c = a + b\n",
    "d = a * b\n",
    "e = d.sum()\n",
    "\n",
    "print(\"c:\", c)\n",
    "print(\"d:\", d)\n",
    "print(\"e:\", e)\n",
    "\n",
    "# Backward pass\n",
    "e.backward()\n",
    "print(\"grad a:\", a.grad)\n",
    "print(\"grad b:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd730884",
   "metadata": {},
   "source": [
    "## Building Neural Network with simplegrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19282851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simplegrad import Tensor, SGD ,MSELoss,Linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b33f226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[[ 0.15805848 -0.09968112 -1.72277728]\n",
      " [-0.44700126  1.3850145  -0.98703172]], grad=[[0. 0. 0.]\n",
      " [0. 0. 0.]], op=, label=)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tensor(data=[[0.67752701 0.93264303]\n",
       " [3.06798948 0.09823291]], grad=[[0. 0.]\n",
       " [0. 0.]], op=matmul, label=)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Tensor.randn((2,3))\n",
    "print(x)\n",
    "l = Linear(3,2,bias=False)\n",
    "l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa90c134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(data=[[-1.31234362  0.16999184]\n",
       "  [-0.79565425  0.15070804]\n",
       "  [-1.06437145  1.23736925]], grad=[[0. 0.]\n",
       "  [0. 0.]\n",
       "  [0. 0.]], op=, label=),\n",
       " None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5b9013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallNetwork():\n",
    "    def __init__(self,in_features, out_features):\n",
    "        self.linear1 = Linear(in_features, out_features,bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    def parameters(self):\n",
    "        return self.linear1.parameters() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37cfeb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  Tensor(data=[[1.03831642]], grad=[[0.]], op=, label=)  w:  Tensor(data=[[0.14872985]], grad=[[0.]], op=, label=)  target:  Tensor(data=[[0.15442864]], grad=[[0.]], op=*, label=)\n",
      "loss:  0.455166391634314 weights 0.789132 bias: 0.009720 grad 1.401022\n",
      "loss:  0.2856464233417963 weights 0.719081 bias: -0.057746 grad 1.109875\n",
      "loss:  0.17926165171156594 weights 0.663587 bias: -0.111192 grad 0.879232\n",
      "loss:  0.11249830961792678 weights 0.619626 bias: -0.153531 grad 0.696519\n",
      "loss:  0.07059998357738197 weights 0.584800 bias: -0.187072 grad 0.551775\n",
      "loss:  0.04430606733607608 weights 0.557211 bias: -0.213642 grad 0.437111\n",
      "loss:  0.02780493002009425 weights 0.535355 bias: -0.234692 grad 0.346275\n",
      "loss:  0.017449396434985175 weights 0.518042 bias: -0.251366 grad 0.274315\n",
      "loss:  0.010950627666576722 weights 0.504326 bias: -0.264576 grad 0.217310\n",
      "loss:  0.006872228889909868 weights 0.493460 bias: -0.275040 grad 0.172151\n",
      "loss:  0.004312769217737044 weights 0.484853 bias: -0.283330 grad 0.136376\n",
      "loss:  0.002706542320319041 weights 0.478034 bias: -0.289898 grad 0.108036\n",
      "loss:  0.0016985307958401875 weights 0.472632 bias: -0.295100 grad 0.085585\n",
      "loss:  0.0010659382056429202 weights 0.468353 bias: -0.299221 grad 0.067799\n",
      "loss:  0.0006689453385431316 weights 0.464963 bias: -0.302486 grad 0.053710\n",
      "loss:  0.0004198065737672693 weights 0.462278 bias: -0.305073 grad 0.042549\n",
      "loss:  0.0002634558449305203 weights 0.460150 bias: -0.307121 grad 0.033706\n",
      "loss:  0.00016533562494076972 weights 0.458465 bias: -0.308745 grad 0.026702\n",
      "loss:  0.00010375882486783887 weights 0.457130 bias: -0.310030 grad 0.021153\n",
      "loss:  6.511539023614429e-05 weights 0.456072 bias: -0.311049 grad 0.016757\n",
      "loss:  4.086412939820752e-05 weights 0.455234 bias: -0.311856 grad 0.013275\n",
      "loss:  2.564489079183239e-05 weights 0.454570 bias: -0.312495 grad 0.010516\n",
      "loss:  1.6093831764194206e-05 weights 0.454045 bias: -0.313002 grad 0.008331\n",
      "loss:  1.0099922942025886e-05 weights 0.453628 bias: -0.313403 grad 0.006600\n",
      "loss:  6.3383565163031755e-06 weights 0.453298 bias: -0.313721 grad 0.005228\n",
      "loss:  3.977729687480526e-06 weights 0.453037 bias: -0.313972 grad 0.004142\n",
      "loss:  2.4962832914124866e-06 weights 0.452830 bias: -0.314172 grad 0.003281\n",
      "loss:  1.566579622189448e-06 weights 0.452666 bias: -0.314330 grad 0.002599\n",
      "loss:  9.831302885782628e-07 weights 0.452536 bias: -0.314455 grad 0.002059\n",
      "loss:  6.169780013921449e-07 weights 0.452433 bias: -0.314554 grad 0.001631\n",
      "loss:  3.871937001882141e-07 weights 0.452351 bias: -0.314633 grad 0.001292\n",
      "loss:  2.429891521694437e-07 weights 0.452286 bias: -0.314695 grad 0.001024\n",
      "loss:  1.5249144819070716e-07 weights 0.452235 bias: -0.314744 grad 0.000811\n",
      "loss:  9.569827115197233e-08 weights 0.452195 bias: -0.314783 grad 0.000642\n",
      "loss:  6.00568701401555e-08 weights 0.452163 bias: -0.314814 grad 0.000509\n",
      "loss:  3.76895800479373e-08 weights 0.452137 bias: -0.314839 grad 0.000403\n",
      "loss:  2.3652655239530028e-08 weights 0.452117 bias: -0.314858 grad 0.000319\n",
      "loss:  1.4843574780308771e-08 weights 0.452101 bias: -0.314873 grad 0.000253\n",
      "loss:  9.3153056190603e-09 weights 0.452088 bias: -0.314886 grad 0.000200\n",
      "loss:  5.845958272243192e-09 weights 0.452078 bias: -0.314895 grad 0.000159\n",
      "loss:  3.668717862658569e-09 weights 0.452070 bias: -0.314903 grad 0.000126\n",
      "loss:  2.3023583352784083e-09 weights 0.452064 bias: -0.314909 grad 0.000100\n",
      "loss:  1.4448791382915891e-09 weights 0.452059 bias: -0.314914 grad 0.000079\n",
      "loss:  9.067553439809038e-10 weights 0.452055 bias: -0.314918 grad 0.000063\n",
      "loss:  5.690477715707883e-10 weights 0.452052 bias: -0.314921 grad 0.000050\n",
      "loss:  3.571143732212748e-10 weights 0.452050 bias: -0.314923 grad 0.000039\n",
      "loss:  2.2411242418057724e-10 weights 0.452048 bias: -0.314925 grad 0.000031\n",
      "loss:  1.4064507742569392e-10 weights 0.452046 bias: -0.314926 grad 0.000025\n",
      "loss:  8.826390538866841e-11 weights 0.452045 bias: -0.314928 grad 0.000020\n",
      "loss:  5.53913235866213e-11 weights 0.452044 bias: -0.314929 grad 0.000015\n",
      "loss:  3.4761647075031866e-11 weights 0.452043 bias: -0.314929 grad 0.000012\n",
      "loss:  2.1815187454972707e-11 weights 0.452043 bias: -0.314930 grad 0.000010\n",
      "loss:  1.3690444605819446e-11 weights 0.452042 bias: -0.314930 grad 0.000008\n",
      "loss:  8.59164166680039e-12 weights 0.452042 bias: -0.314931 grad 0.000006\n",
      "loss:  5.3918122204267546e-12 weights 0.452041 bias: -0.314931 grad 0.000005\n",
      "loss:  3.383711768832727e-12 weights 0.452041 bias: -0.314931 grad 0.000004\n",
      "loss:  2.123498531790714e-12 weights 0.452041 bias: -0.314931 grad 0.000003\n",
      "loss:  1.3326330145563418e-12 weights 0.452041 bias: -0.314932 grad 0.000002\n",
      "loss:  8.36313623342176e-13 weights 0.452041 bias: -0.314932 grad 0.000002\n",
      "loss:  5.248410244729456e-13 weights 0.452041 bias: -0.314932 grad 0.000002\n",
      "loss:  3.2937177309049733e-13 weights 0.452040 bias: -0.314932 grad 0.000001\n",
      "loss:  2.067021437461063e-13 weights 0.452040 bias: -0.314932 grad 0.000001\n",
      "loss:  1.2971899760018206e-13 weights 0.452040 bias: -0.314932 grad 0.000001\n",
      "loss:  8.140708183526157e-14 weights 0.452040 bias: -0.314932 grad 0.000001\n",
      "loss:  5.1088222221987994e-14 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  3.206117195173443e-14 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  2.0120464205380327e-14 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  1.2626895893308381e-14 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  7.924195878824327e-15 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  4.972946714473521e-15 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  3.120846506221296e-15 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  1.95853353675481e-15 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  1.2291067823086438e-15 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  7.713442009968545e-16 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  4.840685015645059e-16 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  3.037843706329718e-16 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  1.9064438983256302e-16 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  1.196417154645014e-16 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  7.50829339203093e-17 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  4.71194095909184e-17 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  2.957048494140274e-17 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  1.8557396280954434e-17 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  1.164596933822438e-17 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  7.308600887845518e-18 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  4.5866210755126074e-18 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  2.8784021134900695e-18 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  1.806383958175598e-18 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  1.13362307760765e-18 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  7.1142200288193e-19 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  4.464634147858799e-19 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  2.8018475046800443e-19 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  1.7583407689491067e-19 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  1.1034728105274277e-19 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  6.925011462287604e-20 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  4.3458938767406846e-20 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  2.7273295626224283e-20 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  1.711576618488904e-20 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  1.0741258097180868e-20 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  6.74083733635609e-21 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "loss:  4.2303136757751056e-21 weights 0.452040 bias: -0.314932 grad 0.000000\n",
      "\n",
      "model final prediction:  0.15442864416136287 true value:  0.1544286441098382 diff:  5.152467341673628e-11\n"
     ]
    }
   ],
   "source": [
    "x = Tensor.randn((1,1))\n",
    "nn   = SmallNetwork(1,1)\n",
    "criterion = MSELoss()\n",
    "optimizer = SGD(nn.parameters(),lr=0.05)\n",
    "w =  Tensor.randn((1,1))\n",
    "\n",
    "target = w * x\n",
    "\n",
    "print(\"x: \",x,\" w: \",w,\" target: \",target)\n",
    "\n",
    "for i in range(100):\n",
    "    pred = nn(x)\n",
    "    loss = criterion(pred,target)\n",
    "    loss.backward()\n",
    "    print(\"loss: \",f\"{loss.data}\",\"weights\",f\"{nn.parameters()[0].data.tolist()[0][0]:3f}\",\"bias:\",f\"{nn.parameters()[1].data.tolist()[0]:3f}\",\"grad\",f\"{nn.parameters()[0].grad.tolist()[0][0]:3f}\")\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "final_pred = nn(x)\n",
    "print('')\n",
    "print(\"model final prediction: \",final_pred.data[0][0], \"true value: \",target.data[0][0] ,\"diff: \",(final_pred.data-target.data).tolist()[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec91515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
