{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb92d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c9980f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, children=(), _op=\"\", label=\"\", grad=None):\n",
    "        # ensure numpy array\n",
    "        self.data = np.array(data, dtype=float)\n",
    "        self.grad = np.zeros_like(self.data) if grad is None else grad\n",
    "        self.children = children\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self._backward = lambda: None\n",
    "        self.shape=self.data.shape\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad}, op={self._op}, label={self.label})\"\n",
    "\n",
    "    # --- elementwise ops ---\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data + other.data, (self, other), _op=\"+\")\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        # Just reverse the order: addition is commutative\n",
    "        return self + other\n",
    "    \n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data - other.data, (self, other), _op=\"-\")\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += -out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        # Just reverse the order: subtraction is commutative\n",
    "        return self - other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data * other.data, (self, other), _op=\"*\")\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        # Just reverse the order: multiplication is commutative\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data / other.data, (self, other), _op=\"/\")\n",
    "\n",
    "        def _backward():\n",
    "            # dX = 1/y * dZ\n",
    "            self.grad += (1 / other.data) * out.grad\n",
    "            # dY = -x / y^2 * dZ\n",
    "            other.grad += (-self.data / (other.data ** 2)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        # Just reverse the order: division is commutative\n",
    "        return self / other\n",
    "\n",
    "\n",
    "    # --- reductions ---\n",
    "    def sum(self):\n",
    "        out = Tensor(self.data.sum(), (self,), _op=\"sum\")\n",
    "        def _backward():\n",
    "            self.grad += np.ones_like(self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def mean(self):\n",
    "        out = Tensor(self.data.mean(), (self,), _op=\"mean\")\n",
    "        def _backward():\n",
    "            self.grad += np.ones_like(self.data) * out.grad / self.data.size\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # --- matrix multiplication ---\n",
    "    def matmul(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data.dot(other.data), (self, other), _op=\"matmul\")\n",
    "        def _backward():\n",
    "            self.grad += out.grad.dot(other.data.T)\n",
    "            other.grad += self.data.T.dot(out.grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def pow(self,other):\n",
    "        if not isinstance(other, (int, float)):\n",
    "            raise Exception(\"other must be int or float\")\n",
    "\n",
    "        out = Tensor(np.power(self.data,other), (self,), _op=\"pow\")\n",
    "        def _backward():\n",
    "            self.grad += (other*self.data**(other-1))*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        out = Tensor(1/(1+np.exp(-self.data)), (self,), _op=\"sigmoid\")\n",
    "        def _backward():\n",
    "            self.grad += out.data*(1-out.data)*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Tensor(np.maximum(0,self.data), (self,), _op=\"relu\")\n",
    "        def _backward():\n",
    "            self.grad += (self.data > 0).astype(float)*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Tensor(np.tanh(self.data), (self,), _op=\"tanh\")\n",
    "        def _backward():\n",
    "            self.grad += (1-out.data**2)*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def exp(self):\n",
    "        out  = Tensor(np.exp(self.data), (self,), _op=\"exp\")\n",
    "        def _backward():\n",
    "            self.grad += out.data*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def cos(self):\n",
    "        out  = Tensor(np.cos(self.data), (self,), _op=\"cos\")\n",
    "        def _backward():\n",
    "            self.grad += -np.sin(self.data)*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sin(self):\n",
    "        out  = Tensor(np.sin(self.data), (self,), _op=\"sin\")\n",
    "        def _backward():\n",
    "            self.grad += np.cos(self.data)*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def ln(self):\n",
    "        out  = Tensor(np.log(self.data), (self,), _op=\"log\")\n",
    "        def _backward():\n",
    "            self.grad += 1/self.data*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def log(self,base):\n",
    "        if base is not isinstance(base,(int,float)):\n",
    "            raise Exception(\"base must be int or float\")\n",
    "        out = Tensor(np.log(self.data) / np.log(base), (self,), _op=f\"log_{base}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 / (self.data * np.log(base))) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v.children:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "        self.grad = np.ones_like(self.data)\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "447945d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[[0. 4. 6.]], grad=[[0. 0. 0.]], op=+, label=)\n"
     ]
    }
   ],
   "source": [
    "a= Tensor([[-1,2,3]])\n",
    "b = Tensor([[1,2,3]])\n",
    "c = a+b\n",
    "print(c) #c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d0e815ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3]],requires_grad=True,dtype=torch.float32)\n",
    "b = torch.tensor([[1,2,3]],requires_grad=True,dtype=torch.float32)\n",
    "c = a*b\n",
    "d=c+5\n",
    "e = d.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a06487b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Tensor grad A:\n",
      " [[0. 0.]\n",
      " [0. 0.]]\n",
      "My Tensor grad B:\n",
      " [[23. 27.]\n",
      " [25. 29.]]\n",
      "PyTorch grad A:\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "PyTorch grad B:\n",
      " tensor([[23., 27.],\n",
      "        [25., 29.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- My Tensor example ---\n",
    "A = Tensor(np.array([[1.0, 2.0], [3.0, 4.0]]))\n",
    "B = Tensor(np.array([[5.0, 6.0], [7.0, 8.0]]))\n",
    "\n",
    "# Addition\n",
    "C = A + B\n",
    "# Subtraction\n",
    "D = C - A\n",
    "# Matrix multiplication\n",
    "E = D.matmul(B)\n",
    "# Sum to make scalar for backward\n",
    "loss = E.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(\"My Tensor grad A:\\n\", A.grad)\n",
    "print(\"My Tensor grad B:\\n\", B.grad)\n",
    "\n",
    "# --- PyTorch equivalent ---\n",
    "A_torch = torch.tensor([[1.0,2.0],[3.0,4.0]], requires_grad=True)\n",
    "B_torch = torch.tensor([[5.0,6.0],[7.0,8.0]], requires_grad=True)\n",
    "\n",
    "C_torch = A_torch + B_torch\n",
    "D_torch = C_torch - A_torch\n",
    "E_torch = D_torch @ B_torch\n",
    "loss_torch = E_torch.sum()\n",
    "loss_torch.backward()\n",
    "\n",
    "print(\"PyTorch grad A:\\n\", A_torch.grad)\n",
    "print(\"PyTorch grad B:\\n\", B_torch.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4b95ae70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Addition + exp ===\n",
      "My Tensor grad A:\n",
      " [[   403.42879349   2980.95798704]\n",
      " [ 22026.46579481 162754.791419  ]]\n",
      "My Tensor grad B:\n",
      " [[   403.42879349   2980.95798704]\n",
      " [ 22026.46579481 162754.791419  ]]\n",
      "PyTorch grad A:\n",
      " tensor([[   403.4288,   2980.9580],\n",
      "        [ 22026.4648, 162754.7969]])\n",
      "PyTorch grad B:\n",
      " tensor([[   403.4288,   2980.9580],\n",
      "        [ 22026.4648, 162754.7969]])\n",
      "Grad difference:\n",
      " [[8.99749926e-06 2.07707717e-05]\n",
      " [9.51056718e-04 5.45599608e-03]]\n",
      "Grad difference:\n",
      " [[8.99749926e-06 2.07707717e-05]\n",
      " [9.51056718e-04 5.45599608e-03]]\n",
      "\n",
      "=== Subtraction + sigmoid ===\n",
      "My Tensor grad A:\n",
      " [[0.01766271 0.01766271]\n",
      " [0.01766271 0.01766271]]\n",
      "My Tensor grad B:\n",
      " [[-0.01766271 -0.01766271]\n",
      " [-0.01766271 -0.01766271]]\n",
      "PyTorch grad A:\n",
      " tensor([[0.0177, 0.0177],\n",
      "        [0.0177, 0.0177]])\n",
      "PyTorch grad B:\n",
      " tensor([[-0.0177, -0.0177],\n",
      "        [-0.0177, -0.0177]])\n",
      "Grad difference:\n",
      " [[3.59709689e-10 3.59709689e-10]\n",
      " [3.59709689e-10 3.59709689e-10]]\n",
      "Grad difference:\n",
      " [[3.59709689e-10 3.59709689e-10]\n",
      " [3.59709689e-10 3.59709689e-10]]\n",
      "\n",
      "=== Matmul + relu ===\n",
      "My Tensor grad A:\n",
      " [[11. 15.]\n",
      " [11. 15.]]\n",
      "My Tensor grad B:\n",
      " [[4. 4.]\n",
      " [6. 6.]]\n",
      "PyTorch grad A:\n",
      " tensor([[11., 15.],\n",
      "        [11., 15.]])\n",
      "PyTorch grad B:\n",
      " tensor([[4., 4.],\n",
      "        [6., 6.]])\n",
      "Grad difference:\n",
      " [[0. 0.]\n",
      " [0. 0.]]\n",
      "Grad difference:\n",
      " [[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Helper to compare gradients\n",
    "def compare_grad(tensor, torch_tensor, tol=1e-6):\n",
    "    diff = np.abs(tensor.grad - torch_tensor.grad.numpy())\n",
    "    print(\"Grad difference:\\n\", diff)\n",
    "    assert np.allclose(tensor.grad, torch_tensor.grad.numpy(), atol=tol), \"Gradients do not match!\"\n",
    "\n",
    "# -------------------\n",
    "# 1️⃣ Matrix Addition + exp\n",
    "print(\"=== Addition + exp ===\")\n",
    "A = Tensor(np.array([[1.0, 2.0], [3.0, 4.0]]))\n",
    "B = Tensor(np.array([[5.0, 6.0], [7.0, 8.0]]))\n",
    "\n",
    "C = (A + B).exp()\n",
    "loss = C.sum()\n",
    "loss.backward()\n",
    "print(\"My Tensor grad A:\\n\", A.grad)\n",
    "print(\"My Tensor grad B:\\n\", B.grad)\n",
    "\n",
    "# PyTorch\n",
    "A_t = torch.tensor([[1.0,2.0],[3.0,4.0]], requires_grad=True)\n",
    "B_t = torch.tensor([[5.0,6.0],[7.0,8.0]], requires_grad=True)\n",
    "C_t = torch.exp(A_t + B_t)\n",
    "loss_t = C_t.sum()\n",
    "loss_t.backward()\n",
    "print(\"PyTorch grad A:\\n\", A_t.grad)\n",
    "print(\"PyTorch grad B:\\n\", B_t.grad)\n",
    "compare_grad(A, A_t)\n",
    "compare_grad(B, B_t)\n",
    "\n",
    "# -------------------\n",
    "# 2️⃣ Matrix Subtraction + sigmoid\n",
    "print(\"\\n=== Subtraction + sigmoid ===\")\n",
    "A = Tensor(np.array([[1.0, 2.0], [3.0, 4.0]]))\n",
    "B = Tensor(np.array([[5.0, 6.0], [7.0, 8.0]]))\n",
    "\n",
    "C = (A - B).sigmoid()\n",
    "loss = C.sum()\n",
    "loss.backward()\n",
    "print(\"My Tensor grad A:\\n\", A.grad)\n",
    "print(\"My Tensor grad B:\\n\", B.grad)\n",
    "\n",
    "# PyTorch\n",
    "A_t = torch.tensor([[1.0,2.0],[3.0,4.0]], requires_grad=True)\n",
    "B_t = torch.tensor([[5.0,6.0],[7.0,8.0]], requires_grad=True)\n",
    "C_t = torch.sigmoid(A_t - B_t)\n",
    "loss_t = C_t.sum()\n",
    "loss_t.backward()\n",
    "print(\"PyTorch grad A:\\n\", A_t.grad)\n",
    "print(\"PyTorch grad B:\\n\", B_t.grad)\n",
    "compare_grad(A, A_t)\n",
    "compare_grad(B, B_t)\n",
    "\n",
    "# -------------------\n",
    "# 3️⃣ Matrix Multiplication + relu\n",
    "print(\"\\n=== Matmul + relu ===\")\n",
    "A = Tensor(np.array([[1.0, 2.0], [3.0, 4.0]]))\n",
    "B = Tensor(np.array([[5.0, 6.0], [7.0, 8.0]]))\n",
    "\n",
    "C = A.matmul(B).relu()\n",
    "loss = C.sum()\n",
    "loss.backward()\n",
    "print(\"My Tensor grad A:\\n\", A.grad)\n",
    "print(\"My Tensor grad B:\\n\", B.grad)\n",
    "\n",
    "# PyTorch\n",
    "A_t = torch.tensor([[1.0,2.0],[3.0,4.0]], requires_grad=True)\n",
    "B_t = torch.tensor([[5.0,6.0],[7.0,8.0]], requires_grad=True)\n",
    "C_t = torch.nn.functional.relu(A_t @ B_t)\n",
    "loss_t = C_t.sum()\n",
    "loss_t.backward()\n",
    "print(\"PyTorch grad A:\\n\", A_t.grad)\n",
    "print(\"PyTorch grad B:\\n\", B_t.grad)\n",
    "compare_grad(A, A_t)\n",
    "compare_grad(B, B_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96b6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
