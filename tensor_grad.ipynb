{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb92d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9980f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, children=(), _op=\"\", label=\"\", grad=None,requires_grad=True):\n",
    "        # ensure numpy array\n",
    "        self.data = np.array(data, dtype=float)\n",
    "        self.grad = None if not requires_grad else (np.zeros_like(self.data) if grad is None else grad)\n",
    "        self.children = children\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self._backward = lambda: None\n",
    "        self.shape=self.data.shape\n",
    "        self.requires_grad=requires_grad\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor(data={self.data}, grad={self.grad}, op={self._op}, label={self.label})\"\n",
    "\n",
    "    # --- elementwise ops ---\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data + other.data, (self, other), _op=\"+\")\n",
    "        if self.requires_grad or other.requires_grad:\n",
    "            out.requires_grad=True\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = np.zeros_like(self.data)\n",
    "                # reduce grad shape if broadcasting happened\n",
    "                grad_self = out.grad\n",
    "                while grad_self.ndim > self.data.ndim:\n",
    "                    grad_self = grad_self.sum(axis=0)\n",
    "                for i, dim in enumerate(self.data.shape):\n",
    "                    if dim == 1:\n",
    "                        grad_self = grad_self.sum(axis=i, keepdims=True)\n",
    "                self.grad += grad_self\n",
    "\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = np.zeros_like(other.data)\n",
    "                grad_other = out.grad\n",
    "                while grad_other.ndim > other.data.ndim:\n",
    "                    grad_other = grad_other.sum(axis=0)\n",
    "\n",
    "                for i, dim in enumerate(other.data.shape):\n",
    "                    if dim == 1:\n",
    "                        grad_other = grad_other.sum(axis=i, keepdims=True)\n",
    "                other.grad += grad_other\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        # Just reverse the order: addition is commutative\n",
    "        return self + other\n",
    "    \n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data - other.data, (self, other), _op=\"-\")\n",
    "        if self.requires_grad or other.requires_grad:\n",
    "            out.requires_grad=True\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += out.grad\n",
    "            if other.requires_grad:\n",
    "                other.grad += -out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        # Just reverse the order: subtraction is commutative\n",
    "        return self - other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data * other.data, (self, other), _op=\"*\")\n",
    "        if self.requires_grad or other.requires_grad:\n",
    "            out.requires_grad=True\n",
    "\n",
    "        def _backward():\n",
    "            # gradient wrt self\n",
    "            grad_self = other.data * out.grad\n",
    "            while grad_self.ndim > self.data.ndim:\n",
    "                grad_self = grad_self.sum(axis=0)\n",
    "            for i, dim in enumerate(self.data.shape):\n",
    "                if dim == 1:\n",
    "                    grad_self = grad_self.sum(axis=i, keepdims=True)\n",
    "            if self.requires_grad:\n",
    "                self.grad += grad_self\n",
    "\n",
    "            # gradient wrt other\n",
    "            grad_other = self.data * out.grad\n",
    "            while grad_other.ndim > other.data.ndim:\n",
    "                grad_other = grad_other.sum(axis=0)\n",
    "            for i, dim in enumerate(other.data.shape):\n",
    "                if dim == 1:\n",
    "                    grad_other = grad_other.sum(axis=i, keepdims=True)\n",
    "            if other.requires_grad:\n",
    "                other.grad += grad_other\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        # Just reverse the order: multiplication is commutative\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data / other.data, (self, other), _op=\"/\")\n",
    "        if self.requires_grad or other.requires_grad:\n",
    "            out.requires_grad=True\n",
    "\n",
    "        def _backward():\n",
    "            # dX = 1/y * dZ\n",
    "            if self.requires_grad:\n",
    "                self.grad += (1 / other.data) * out.grad\n",
    "            # dY = -x / y^2 * dZ\n",
    "            if other.requires_grad:\n",
    "                other.grad += (-self.data / (other.data ** 2)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        # Just reverse the order: division is commutative\n",
    "        return self / other\n",
    "\n",
    "\n",
    "    # --- reductions ---\n",
    "    def sum(self):\n",
    "        out = Tensor(self.data.sum(), (self,), _op=\"sum\")\n",
    "        if self.requires_grad :\n",
    "            out.requires_grad=True\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += np.ones_like(self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def mean(self):\n",
    "        out = Tensor(self.data.mean(), (self,), _op=\"mean\")\n",
    "        if self.requires_grad :\n",
    "            out.requires_grad=True\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += np.ones_like(self.data) * out.grad / self.data.size\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # --- matrix multiplication ---\n",
    "    def matmul(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data.dot(other.data), (self, other), _op=\"matmul\")\n",
    "        if self.requires_grad or other.requires_grad:\n",
    "            out.requires_grad=True\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += out.grad.dot(other.data.T)\n",
    "            if out.requires_grad:\n",
    "                other.grad += self.data.T.dot(out.grad)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def __matmul__(self, other):\n",
    "        return self.matmul(other)\n",
    "\n",
    "    def __rmatmul__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return other.matmul(self)\n",
    "    \n",
    "    def pow(self,other):\n",
    "        if not isinstance(other, (int, float)):\n",
    "            raise Exception(\"other must be int or float\")\n",
    "        if self.requires_grad :\n",
    "            out.requires_grad=True\n",
    "\n",
    "        out = Tensor(np.power(self.data,other), (self,), _op=\"pow\")\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += (other*self.data**(other-1))*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        out = Tensor(1/(1+np.exp(-self.data)), (self,), _op=\"sigmoid\")\n",
    "        if self.requires_grad :\n",
    "            out.requires_grad=True\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += out.data*(1-out.data)*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Tensor(np.maximum(0,self.data), (self,), _op=\"relu\")\n",
    "        if self.requires_grad :\n",
    "            out.requires_grad=True\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += (self.data > 0).astype(float)*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Tensor(np.tanh(self.data), (self,), _op=\"tanh\")\n",
    "        if self.requires_grad :\n",
    "            out.requires_grad=True\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += (1-out.data**2)*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def exp(self):\n",
    "        out  = Tensor(np.exp(self.data), (self,), _op=\"exp\")\n",
    "        if self.requires_grad :\n",
    "            out.requires_grad=True\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += out.data*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def cos(self):\n",
    "        out  = Tensor(np.cos(self.data), (self,), _op=\"cos\")\n",
    "        if self.requires_grad :\n",
    "            out.requires_grad=True\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += -np.sin(self.data)*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sin(self):\n",
    "        out  = Tensor(np.sin(self.data), (self,), _op=\"sin\")\n",
    "        if self.requires_grad :\n",
    "            out.requires_grad=True\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += np.cos(self.data)*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def ln(self):\n",
    "        out  = Tensor(np.log(self.data), (self,), _op=\"log\")\n",
    "        if self.requires_grad :\n",
    "            out.requires_grad=True\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += 1/self.data*out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    def log(self,base):\n",
    "        if base is not isinstance(base,(int,float)):\n",
    "            raise Exception(\"base must be int or float\")\n",
    "        out = Tensor(np.log(self.data) / np.log(base), (self,), _op=f\"log_{base}\")\n",
    "        if self.requires_grad :\n",
    "            out.requires_grad=True\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                self.grad += (1 / (self.data * np.log(base))) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    @staticmethod\n",
    "    def _match_shape(grad, shape):\n",
    "        \"\"\"\n",
    "        Reduce grad to match the given shape by summing over broadcasted axes.\n",
    "        \"\"\"\n",
    "        while grad.ndim > len(shape):\n",
    "            grad = grad.sum(axis=0)\n",
    "\n",
    "        for i, dim in enumerate(shape):\n",
    "            if dim == 1:\n",
    "                grad = grad.sum(axis=i, keepdims=True)\n",
    "\n",
    "        return grad\n",
    "\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        if not self.requires_grad:\n",
    "                return print(\"no grad please set requires_grad=True\")\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v.children:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "        self.grad = np.ones_like(self.data)\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "46fac77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[[6. 7. 8.]], grad=[[0. 0. 0.]], op=+, label=)\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([[1,2,3]],requires_grad=True)\n",
    "c = a+5\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "5d1e1780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before summing grad_other:  2\n",
      "other.data.ndim:  0\n",
      "after summing grad_other:  1\n",
      "before summing grad_other:  1\n",
      "other.data.ndim:  0\n",
      "after summing grad_other:  0\n",
      "other.data.shape ()\n",
      "[[1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "c.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "447945d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[[0. 4. 6.]], grad=[[0. 0. 0.]], op=+, label=)\n"
     ]
    }
   ],
   "source": [
    "a= Tensor([[-1,2,3]],requires_grad=True)\n",
    "b = Tensor([[1,2,3]])\n",
    "c = a+b\n",
    "print(c) #c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "023c26fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "438bf437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=[[1. 2. 3.]], grad=[[1. 1. 1.]], op=, label=)"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "d0e815ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([[1,2,3]],requires_grad=True)\n",
    "b = Tensor([[1,2,3]],requires_grad=True)\n",
    "c = a*b\n",
    "d=c+5\n",
    "e = d.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "76d5e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "3cb288a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=[[ 6.  9. 14.]], grad=[[1. 1. 1.]], op=+, label=)"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "a06487b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Tensor grad A:\n",
      " [[0. 0.]\n",
      " [0. 0.]]\n",
      "My Tensor grad B:\n",
      " [[23. 27.]\n",
      " [25. 29.]]\n",
      "PyTorch grad A:\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "PyTorch grad B:\n",
      " tensor([[23., 27.],\n",
      "        [25., 29.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- My Tensor example ---\n",
    "A = Tensor(np.array([[1.0, 2.0], [3.0, 4.0]]))\n",
    "B = Tensor(np.array([[5.0, 6.0], [7.0, 8.0]]))\n",
    "\n",
    "# Addition\n",
    "C = A + B\n",
    "# Subtraction\n",
    "D = C - A\n",
    "# Matrix multiplication\n",
    "E = D.matmul(B)\n",
    "# Sum to make scalar for backward\n",
    "loss = E.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(\"My Tensor grad A:\\n\", A.grad)\n",
    "print(\"My Tensor grad B:\\n\", B.grad)\n",
    "\n",
    "# --- PyTorch equivalent ---\n",
    "A_torch = torch.tensor([[1.0,2.0],[3.0,4.0]], requires_grad=True)\n",
    "B_torch = torch.tensor([[5.0,6.0],[7.0,8.0]], requires_grad=True)\n",
    "\n",
    "C_torch = A_torch + B_torch\n",
    "D_torch = C_torch - A_torch\n",
    "E_torch = D_torch @ B_torch\n",
    "loss_torch = E_torch.sum()\n",
    "loss_torch.backward()\n",
    "\n",
    "print(\"PyTorch grad A:\\n\", A_torch.grad)\n",
    "print(\"PyTorch grad B:\\n\", B_torch.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "4b95ae70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Addition + exp ===\n",
      "My Tensor grad A:\n",
      " [[   403.42879349   2980.95798704]\n",
      " [ 22026.46579481 162754.791419  ]]\n",
      "My Tensor grad B:\n",
      " [[   403.42879349   2980.95798704]\n",
      " [ 22026.46579481 162754.791419  ]]\n",
      "PyTorch grad A:\n",
      " tensor([[   403.4288,   2980.9580],\n",
      "        [ 22026.4648, 162754.7969]])\n",
      "PyTorch grad B:\n",
      " tensor([[   403.4288,   2980.9580],\n",
      "        [ 22026.4648, 162754.7969]])\n",
      "Grad difference:\n",
      " [[8.99749926e-06 2.07707717e-05]\n",
      " [9.51056718e-04 5.45599608e-03]]\n",
      "Grad difference:\n",
      " [[8.99749926e-06 2.07707717e-05]\n",
      " [9.51056718e-04 5.45599608e-03]]\n",
      "\n",
      "=== Subtraction + sigmoid ===\n",
      "My Tensor grad A:\n",
      " [[0.01766271 0.01766271]\n",
      " [0.01766271 0.01766271]]\n",
      "My Tensor grad B:\n",
      " [[-0.01766271 -0.01766271]\n",
      " [-0.01766271 -0.01766271]]\n",
      "PyTorch grad A:\n",
      " tensor([[0.0177, 0.0177],\n",
      "        [0.0177, 0.0177]])\n",
      "PyTorch grad B:\n",
      " tensor([[-0.0177, -0.0177],\n",
      "        [-0.0177, -0.0177]])\n",
      "Grad difference:\n",
      " [[3.59709689e-10 3.59709689e-10]\n",
      " [3.59709689e-10 3.59709689e-10]]\n",
      "Grad difference:\n",
      " [[3.59709689e-10 3.59709689e-10]\n",
      " [3.59709689e-10 3.59709689e-10]]\n",
      "\n",
      "=== Matmul + relu ===\n",
      "My Tensor grad A:\n",
      " [[11. 15.]\n",
      " [11. 15.]]\n",
      "My Tensor grad B:\n",
      " [[4. 4.]\n",
      " [6. 6.]]\n",
      "PyTorch grad A:\n",
      " tensor([[11., 15.],\n",
      "        [11., 15.]])\n",
      "PyTorch grad B:\n",
      " tensor([[4., 4.],\n",
      "        [6., 6.]])\n",
      "Grad difference:\n",
      " [[0. 0.]\n",
      " [0. 0.]]\n",
      "Grad difference:\n",
      " [[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Helper to compare gradients\n",
    "def compare_grad(tensor, torch_tensor, tol=1e-6):\n",
    "    diff = np.abs(tensor.grad - torch_tensor.grad.numpy())\n",
    "    print(\"Grad difference:\\n\", diff)\n",
    "    assert np.allclose(tensor.grad, torch_tensor.grad.numpy(), atol=tol), \"Gradients do not match!\"\n",
    "\n",
    "# -------------------\n",
    "# 1️⃣ Matrix Addition + exp\n",
    "print(\"=== Addition + exp ===\")\n",
    "A = Tensor(np.array([[1.0, 2.0], [3.0, 4.0]]))\n",
    "B = Tensor(np.array([[5.0, 6.0], [7.0, 8.0]]))\n",
    "\n",
    "C = (A + B).exp()\n",
    "loss = C.sum()\n",
    "loss.backward()\n",
    "print(\"My Tensor grad A:\\n\", A.grad)\n",
    "print(\"My Tensor grad B:\\n\", B.grad)\n",
    "\n",
    "# PyTorch\n",
    "A_t = torch.tensor([[1.0,2.0],[3.0,4.0]], requires_grad=True)\n",
    "B_t = torch.tensor([[5.0,6.0],[7.0,8.0]], requires_grad=True)\n",
    "C_t = torch.exp(A_t + B_t)\n",
    "loss_t = C_t.sum()\n",
    "loss_t.backward()\n",
    "print(\"PyTorch grad A:\\n\", A_t.grad)\n",
    "print(\"PyTorch grad B:\\n\", B_t.grad)\n",
    "compare_grad(A, A_t)\n",
    "compare_grad(B, B_t)\n",
    "\n",
    "# -------------------\n",
    "# 2️⃣ Matrix Subtraction + sigmoid\n",
    "print(\"\\n=== Subtraction + sigmoid ===\")\n",
    "A = Tensor(np.array([[1.0, 2.0], [3.0, 4.0]]))\n",
    "B = Tensor(np.array([[5.0, 6.0], [7.0, 8.0]]))\n",
    "\n",
    "C = (A - B).sigmoid()\n",
    "loss = C.sum()\n",
    "loss.backward()\n",
    "print(\"My Tensor grad A:\\n\", A.grad)\n",
    "print(\"My Tensor grad B:\\n\", B.grad)\n",
    "\n",
    "# PyTorch\n",
    "A_t = torch.tensor([[1.0,2.0],[3.0,4.0]], requires_grad=True)\n",
    "B_t = torch.tensor([[5.0,6.0],[7.0,8.0]], requires_grad=True)\n",
    "C_t = torch.sigmoid(A_t - B_t)\n",
    "loss_t = C_t.sum()\n",
    "loss_t.backward()\n",
    "print(\"PyTorch grad A:\\n\", A_t.grad)\n",
    "print(\"PyTorch grad B:\\n\", B_t.grad)\n",
    "compare_grad(A, A_t)\n",
    "compare_grad(B, B_t)\n",
    "\n",
    "# -------------------\n",
    "# 3️⃣ Matrix Multiplication + relu\n",
    "print(\"\\n=== Matmul + relu ===\")\n",
    "A = Tensor(np.array([[1.0, 2.0], [3.0, 4.0]]))\n",
    "B = Tensor(np.array([[5.0, 6.0], [7.0, 8.0]]))\n",
    "\n",
    "C = A.matmul(B).relu()\n",
    "loss = C.sum()\n",
    "loss.backward()\n",
    "print(\"My Tensor grad A:\\n\", A.grad)\n",
    "print(\"My Tensor grad B:\\n\", B.grad)\n",
    "\n",
    "# PyTorch\n",
    "A_t = torch.tensor([[1.0,2.0],[3.0,4.0]], requires_grad=True)\n",
    "B_t = torch.tensor([[5.0,6.0],[7.0,8.0]], requires_grad=True)\n",
    "C_t = torch.nn.functional.relu(A_t @ B_t)\n",
    "loss_t = C_t.sum()\n",
    "loss_t.backward()\n",
    "print(\"PyTorch grad A:\\n\", A_t.grad)\n",
    "print(\"PyTorch grad B:\\n\", B_t.grad)\n",
    "compare_grad(A, A_t)\n",
    "compare_grad(B, B_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "45b567ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[[-1.75973585  3.07053341]\n",
      " [ 1.27719987 -1.06032032]], grad=[[0. 0.]\n",
      " [0. 0.]], op=matmul, label=)\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(np.random.randn(2,3))\n",
    "y = Tensor(np.random.randn(3,2))\n",
    "print(x@y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "4f96b6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.weight = Tensor(np.random.randn(in_features, out_features))\n",
    "        self.bias = Tensor(np.random.randn(out_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.weight + self.bias\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    def parameters(self):\n",
    "        return [self.weight, self.bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "6866a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[[-2.45803963  3.11235204]\n",
      " [-1.14434508 -2.96598593]], grad=[[0. 0.]\n",
      " [0. 0.]], op=+, label=)\n"
     ]
    }
   ],
   "source": [
    "layer = Linear(3,2)\n",
    "x = Tensor(np.random.randn(2,3))\n",
    "y = layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "817b7282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(data=[[-0.95314974 -0.16823208]\n",
       "  [-0.31622076  0.4187029 ]\n",
       "  [ 0.27073493 -2.23905362]], grad=[[0. 0.]\n",
       "  [0. 0.]\n",
       "  [0. 0.]], op=, label=),\n",
       " Tensor(data=[-1.58061987  0.06439794], grad=[0. 0.], op=, label=)]"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea343674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self,in_features, hidden_features, out_features):\n",
    "        self.linear1 = Linear(in_features, out_features)\n",
    "        self.linear2 = Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = x.relu()\n",
    "        x = self.linear2(x)\n",
    "        x = x.sigmoid()\n",
    "        return x\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    def parameters(self):\n",
    "        return self.linear1.parameters() + self.linear2.parameters()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "a62967b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Network(1,2,1)\n",
    "x = Tensor(np.random.randn(1,1))\n",
    "y = nn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "ea7d235b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(data=[[-0.75620544]], grad=[[0.]], op=, label=),\n",
       " Tensor(data=[1.20874395], grad=[0.], op=, label=)]"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "f0abf252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return (y_true - y_pred).pow(2).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "7852abdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=[[12.]], grad=[[0.]], op=*, label=)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = Tensor(data=[[6]])\n",
    "\n",
    "w =  Tensor(data=[[2]])\n",
    "target = input * w\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "5ee36486",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn   = Network(1,5,1)\n",
    "predictions = nn(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "e0cf3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MSE()\n",
    "loss = criterion(predictions,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "ecb13201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=121.01510048403132, grad=1.0, op=mean, label=)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "061266b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "520a6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = nn.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "f9e24410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(data=[[ 0.44723624  1.82550884  0.65553088 -0.32617895 -0.27775964]], grad=[[ 2.99988889 -1.83103454  0.37204904  0.          0.        ]], op=, label=),\n",
       " Tensor(data=[-1.95929713  0.47129701 -1.69992979 -0.8358522  -0.34773144], grad=[ 0.49998148 -0.30517242  0.06200817  0.          0.        ], op=, label=),\n",
       " Tensor(data=[[-0.74673449]\n",
       "  [ 0.45578243]\n",
       "  [-0.09261071]\n",
       "  [ 0.75263338]\n",
       "  [ 0.94824092]], grad=[[-0.48483999]\n",
       "  [-7.64925631]\n",
       "  [-1.4952924 ]\n",
       "  [ 0.        ]\n",
       "  [ 0.        ]], op=, label=),\n",
       " Tensor(data=[-1.02810542], grad=[-0.66955724], op=, label=)]"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "829707e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.data -= p.grad * 0.01\n",
    "    p.grad = np.zeros_like(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "a0941dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(data=[[ 0.38723846  1.86212954  0.6480899  -0.32617895 -0.27775964]], grad=[[0. 0. 0. 0. 0.]], op=, label=),\n",
       " Tensor(data=[-1.96929676  0.47740046 -1.70116996 -0.8358522  -0.34773144], grad=[0. 0. 0. 0. 0.], op=, label=),\n",
       " Tensor(data=[[-0.73703769]\n",
       "  [ 0.60876755]\n",
       "  [-0.06270486]\n",
       "  [ 0.75263338]\n",
       "  [ 0.94824092]], grad=[[0.]\n",
       "  [0.]\n",
       "  [0.]\n",
       "  [0.]\n",
       "  [0.]], op=, label=),\n",
       " Tensor(data=[-1.01471427], grad=[0.], op=, label=)]"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c1f203",
   "metadata": {},
   "source": [
    "## ALL IN ONE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "428f94aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallNetwork():\n",
    "    def __init__(self,in_features, out_features):\n",
    "        self.linear1 = Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        return x\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    def parameters(self):\n",
    "        return self.linear1.parameters() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "84efa792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  330.129313 weights -0.981568 grad -218.033532\n",
      "loss:  22.316742 weights 1.198767 grad -56.688718\n",
      "loss:  1.508612 weights 1.765654 grad -14.739067\n",
      "loss:  0.101982 weights 1.913045 grad -3.832157\n",
      "loss:  0.006894 weights 1.951366 grad -0.996361\n",
      "loss:  0.000466 weights 1.961330 grad -0.259054\n",
      "loss:  0.000032 weights 1.963920 grad -0.067354\n",
      "loss:  0.000002 weights 1.964594 grad -0.017512\n",
      "loss:  0.000000 weights 1.964769 grad -0.004553\n",
      "loss:  0.000000 weights 1.964815 grad -0.001184\n",
      "loss:  0.000000 weights 1.964826 grad -0.000308\n",
      "loss:  0.000000 weights 1.964830 grad -0.000080\n",
      "loss:  0.000000 weights 1.964830 grad -0.000021\n",
      "loss:  0.000000 weights 1.964831 grad -0.000005\n",
      "loss:  0.000000 weights 1.964831 grad -0.000001\n",
      "loss:  0.000000 weights 1.964831 grad -0.000000\n",
      "loss:  0.000000 weights 1.964831 grad -0.000000\n",
      "loss:  0.000000 weights 1.964831 grad -0.000000\n",
      "loss:  0.000000 weights 1.964831 grad -0.000000\n",
      "loss:  0.000000 weights 1.964831 grad -0.000000\n",
      "loss:  0.000000 weights 1.964831 grad -0.000000\n",
      "loss:  0.000000 weights 1.964831 grad -0.000000\n",
      "loss:  0.000000 weights 1.964831 grad -0.000000\n",
      "loss:  0.000000 weights 1.964831 grad -0.000000\n",
      "loss:  0.000000 weights 1.964831 grad -0.000000\n",
      "loss:  0.000000 weights 1.964831 grad -0.000000\n",
      "loss:  0.000000 weights 1.964831 grad -0.000000\n",
      "loss:  0.000000 weights 1.964831 grad -0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n",
      "loss:  0.000000 weights 1.964831 grad 0.000000\n"
     ]
    }
   ],
   "source": [
    "nn   = SmallNetwork(1,1)\n",
    "input = Tensor(data=[[6]])\n",
    "target = input * w\n",
    "criterion = MSE()\n",
    "\n",
    "for i in range(200):\n",
    "\n",
    "    predictions = nn(input)\n",
    "    loss = criterion(predictions,target)\n",
    "    loss.backward()\n",
    "    print(\"loss: \",f\"{loss.data:4f}\",\"weights\",f\"{nn.parameters()[0].data.tolist()[0][0]:3f}\",\"grad\",f\"{nn.parameters()[0].grad.tolist()[0][0]:3f}\")\n",
    "    for p in nn.parameters():\n",
    "        p.data -= p.grad * 0.01\n",
    "        p.grad = np.zeros_like(p.grad)\n",
    "\n",
    "predictions = nn(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "e1a5ad2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.parameters()[0].grad.tolist()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64acc374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
